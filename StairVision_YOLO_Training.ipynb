{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ StairVision YOLO Training Pipeline\n",
    "\n",
    "**Project**: CS663 Mobile Vision - Stair Detection Android App\n",
    "\n",
    "**Goal**: Train YOLOv8 model on stairs dataset and export to TensorFlow Lite for Android\n",
    "\n",
    "**Requirements**:\n",
    "- Dataset uploaded to Google Drive: `MyDrive/stairs_yolo_dataset.zip`\n",
    "- GPU Runtime: T4 GPU (Runtime â†’ Change runtime type â†’ T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Training Steps\n",
    "1. Install dependencies\n",
    "2. Mount Google Drive\n",
    "3. Extract and verify dataset\n",
    "4. Train YOLOv8 model (100 epochs)\n",
    "5. Evaluate performance\n",
    "6. Export to TFLite Float32\n",
    "7. Save model to Google Drive\n",
    "\n",
    "**Estimated Time**: 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 1: Install Dependencies and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (with PyTorch 2.6 compatibility fix)\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 -q\n",
    "!pip install ultralytics==8.1.0 -q\n",
    "!pip install tensorflow==2.15.0 -q\n",
    "\n",
    "print(\"âœ… Package installation complete\")\n",
    "\n",
    "# Import libraries\n",
    "from google.colab import drive\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import Image, display\n",
    "import torch\n",
    "\n",
    "# Fix for PyTorch 2.6 weights_only issue\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nâœ… Google Drive mounted successfully\")\n",
    "print(\"âœ… Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 2: Extract and Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location in Google Drive\n",
    "dataset_zip = '/content/drive/MyDrive/stairs_yolo_dataset.zip'\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"âŒ ERROR: Dataset not found!\")\n",
    "    print(f\"   Expected location: {dataset_zip}\")\n",
    "    print(\"\\nğŸ“‹ Upload Instructions:\")\n",
    "    print(\"   1. Go to drive.google.com\")\n",
    "    print(\"   2. Upload stairs_yolo_dataset.zip to MyDrive/\")\n",
    "    print(\"   3. Refresh and run this cell again\")\n",
    "    raise FileNotFoundError(\"Dataset not found in Google Drive\")\n",
    "\n",
    "print(f\"âœ… Found dataset: {dataset_zip}\")\n",
    "print(f\"   Size: {os.path.getsize(dataset_zip) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Extract dataset\n",
    "print(\"\\nğŸ“¦ Extracting dataset...\")\n",
    "!unzip -o -q {dataset_zip} -d /content/\n",
    "\n",
    "dataset_path = '/content/stairs_yolo_dataset'\n",
    "\n",
    "# Verify directory structure\n",
    "print(\"\\nğŸ“ Dataset structure:\")\n",
    "!ls -R {dataset_path} | head -40\n",
    "\n",
    "# Count files\n",
    "train_imgs = len([f for f in os.listdir(f'{dataset_path}/images/train') if f.endswith('.jpg')])\n",
    "val_imgs = len([f for f in os.listdir(f'{dataset_path}/images/val') if f.endswith('.jpg')])\n",
    "train_labels = len([f for f in os.listdir(f'{dataset_path}/labels/train') if f.endswith('.txt')])\n",
    "val_labels = len([f for f in os.listdir(f'{dataset_path}/labels/val') if f.endswith('.txt')])\n",
    "\n",
    "print(\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Training:   {train_imgs} images | {train_labels} labels\")\n",
    "print(f\"   Validation: {val_imgs} images | {val_labels} labels\")\n",
    "print(f\"   Total:      {train_imgs + val_imgs} images\")\n",
    "\n",
    "# Verify match\n",
    "if train_imgs == train_labels and val_imgs == val_labels:\n",
    "    print(\"\\nâœ… Dataset verification PASSED - Ready for training!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Warning: Image/label count mismatch\")\n",
    "    print(f\"   Train: {train_imgs} images vs {train_labels} labels\")\n",
    "    print(f\"   Val: {val_imgs} images vs {val_labels} labels\")\n",
    "\n",
    "# Fix data.yaml paths for Colab\n",
    "print(\"\\nğŸ”§ Updating data.yaml paths for Colab...\")\n",
    "yaml_path = f'{dataset_path}/data.yaml'\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(f'''# Stair Detection Dataset\\n''')\n",
    "    f.write(f'''path: {dataset_path}\\n''')\n",
    "    f.write(f'''train: images/train\\n''')\n",
    "    f.write(f'''val: images/val\\n''')\n",
    "    f.write(f'''\\n''')\n",
    "    f.write(f'''# Number of classes\\n''')\n",
    "    f.write(f'''nc: 1\\n''')\n",
    "    f.write(f'''\\n''')\n",
    "    f.write(f'''# Class names\\n''')\n",
    "    f.write(f'''names:\\n''')\n",
    "    f.write(f'''  0: stairs\\n''')\n",
    "print(f\"âœ… data.yaml updated with Colab paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 3: Initialize YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained YOLOv8 nano model\n",
    "print(\"ğŸ“¥ Downloading YOLOv8n pretrained weights...\")\n",
    "\n",
    "# Fix for PyTorch 2.6 weights_only security issue\n",
    "import torch.serialization\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "\n",
    "# Add safe globals for YOLO model loading\n",
    "torch.serialization.add_safe_globals([DetectionModel])\n",
    "print(\"ğŸ”§ Applied PyTorch 2.6 compatibility fix\")\n",
    "\n",
    "# Now load the model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "print(\"\\nâœ… YOLOv8n model loaded successfully\")\n",
    "print(f\"   Model type: YOLOv8 Nano\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "print(f\"   Optimized for: Mobile/Edge devices\")\n",
    "print(f\"\\nğŸ¯ Ready to train on stairs dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 4: Train the Model (â±ï¸ Takes 1-2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¥ Starting training...\")\n",
    "print(\"   This will take approximately 1-2 hours\")\n",
    "print(\"   You can monitor progress below\\n\")\n",
    "\n",
    "# Training configuration\n",
    "results = model.train(\n",
    "    data='/content/stairs_yolo_dataset/data.yaml',\n",
    "    epochs=100,              # Number of training epochs\n",
    "    imgsz=640,              # Image size (must match Android: 640x640)\n",
    "    batch=16,               # Batch size (reduce to 8 if GPU memory issues)\n",
    "    device=0,               # Use GPU 0\n",
    "    project='stair_detection',\n",
    "    name='yolov8n_stairs_v1',\n",
    "    patience=20,            # Early stopping patience\n",
    "    save=True,              # Save checkpoints\n",
    "    plots=True,             # Generate training plots\n",
    "    single_cls=True,        # Single class optimization (stairs only)\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Data augmentation (helps with small dataset)\n",
    "    hsv_h=0.015,           # Hue augmentation\n",
    "    hsv_s=0.7,             # Saturation augmentation\n",
    "    hsv_v=0.4,             # Value/brightness augmentation\n",
    "    degrees=10,            # Random rotation (Â±10 degrees)\n",
    "    translate=0.1,         # Random translation (10%)\n",
    "    scale=0.5,             # Random scale (50% variation)\n",
    "    shear=0,               # Shearing (disabled)\n",
    "    perspective=0.0,       # Perspective transform (disabled)\n",
    "    flipud=0.0,            # Vertical flip probability (disabled)\n",
    "    fliplr=0.5,            # Horizontal flip probability (50%)\n",
    "    mosaic=1.0,            # Mosaic augmentation (100%)\n",
    "    mixup=0.1,             # Mixup augmentation (10%)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"   Results saved to: /content/stair_detection/yolov8n_stairs_v1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 5: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on validation set\n",
    "print(\"ğŸ“Š Evaluating model on validation set...\\n\")\n",
    "metrics = model.val()\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ¯ Detection Metrics:\")\n",
    "print(f\"   mAP@50:     {metrics.box.map50:.4f} ({metrics.box.map50*100:.2f}%)\")\n",
    "print(f\"   mAP@50-95:  {metrics.box.map:.4f} ({metrics.box.map*100:.2f}%)\")\n",
    "print(f\"   Precision:  {metrics.box.p:.4f} ({metrics.box.p*100:.2f}%)\")\n",
    "print(f\"   Recall:     {metrics.box.r:.4f} ({metrics.box.r*100:.2f}%)\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nğŸ’¡ Performance Assessment:\")\n",
    "if metrics.box.map50 > 0.7:\n",
    "    print(\"   âœ… EXCELLENT performance (>70%)\")\n",
    "    print(\"   â†’ Model is ready for production deployment\")\n",
    "elif metrics.box.map50 > 0.5:\n",
    "    print(\"   âœ… GOOD performance (>50%)\")\n",
    "    print(\"   â†’ Model is suitable for deployment\")\n",
    "elif metrics.box.map50 > 0.3:\n",
    "    print(\"   âš ï¸  ACCEPTABLE performance (>30%)\")\n",
    "    print(\"   â†’ Consider training longer or adding more data\")\n",
    "else:\n",
    "    print(\"   âŒ POOR performance (<30%)\")\n",
    "    print(\"   â†’ Needs more training epochs or more training data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 6: Display Training Results & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results directory\n",
    "results_dir = '/content/stair_detection/yolov8n_stairs_v1'\n",
    "\n",
    "# Display training curves\n",
    "print(\"ğŸ“ˆ Training Curves:\")\n",
    "display(Image(f'{results_dir}/results.png'))\n",
    "\n",
    "print(\"\\nğŸ“Š Confusion Matrix:\")\n",
    "display(Image(f'{results_dir}/confusion_matrix.png'))\n",
    "\n",
    "print(\"\\nğŸ¯ Validation Batch Predictions:\")\n",
    "display(Image(f'{results_dir}/val_batch0_pred.jpg'))\n",
    "\n",
    "print(\"\\nâœ… All training visualizations displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 7: Test Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on validation images\n",
    "print(\"ğŸ§ª Running predictions on validation set...\")\n",
    "\n",
    "test_images = '/content/stairs_yolo_dataset/images/val'\n",
    "results = model.predict(test_images, save=True, conf=0.3)\n",
    "\n",
    "print(f\"âœ… Predictions saved to: /content/runs/detect/predict/\")\n",
    "\n",
    "# Display sample predictions\n",
    "from glob import glob\n",
    "pred_images = sorted(glob('/content/runs/detect/predict/*.jpg'))\n",
    "\n",
    "if pred_images:\n",
    "    print(f\"\\nğŸ–¼ï¸  Displaying sample predictions ({len(pred_images)} total):\\n\")\n",
    "    \n",
    "    # Show first 3 predictions\n",
    "    for i, img_path in enumerate(pred_images[:3]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        display(Image(img_path, width=600))\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸  No prediction images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 8: Export to TensorFlow Lite (Float32 - CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best trained model\n",
    "best_model_path = '/content/stair_detection/yolov8n_stairs_v1/weights/best.pt'\n",
    "print(f\"ğŸ“¦ Loading best model from: {best_model_path}\")\n",
    "model = YOLO(best_model_path)\n",
    "\n",
    "print(\"\\nğŸ”„ Exporting to TensorFlow Lite (Float32)...\")\n",
    "print(\"   IMPORTANT: Using float32 (NOT int8) for Android compatibility\\n\")\n",
    "\n",
    "# Export to TFLite with float32 precision\n",
    "tflite_path = model.export(\n",
    "    format='tflite',\n",
    "    imgsz=640,\n",
    "    int8=False,  # CRITICAL: Use float32, not int8 quantization\n",
    "    data='/content/stairs_yolo_dataset/data.yaml'\n",
    ")\n",
    "\n",
    "print(f\"âœ… TFLite model exported successfully!\")\n",
    "print(f\"   Path: {tflite_path}\")\n",
    "print(f\"   Size: {os.path.getsize(tflite_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Rename to Android-expected name\n",
    "final_model_path = '/content/stair_detection/yolov8n_stairs_v1/weights/stair_yolo_best_float32.tflite'\n",
    "shutil.copy(tflite_path, final_model_path)\n",
    "\n",
    "print(f\"âœ… Model renamed to: stair_yolo_best_float32.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 9: Verify TFLite Model Format (CRITICAL CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model\n",
    "print(\"ğŸ” Verifying TFLite model format...\\n\")\n",
    "interpreter = tf.lite.Interpreter(model_path=final_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” TFLite Model Specification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“¥ Input Tensor:\")\n",
    "print(f\"   Shape:     {input_details[0]['shape']}\")\n",
    "print(f\"   Type:      {input_details[0]['dtype']}\")\n",
    "print(f\"   Expected:  [1, 640, 640, 3] float32\")\n",
    "\n",
    "print(\"\\nğŸ“¤ Output Tensor:\")\n",
    "print(f\"   Shape:     {output_details[0]['shape']}\")\n",
    "print(f\"   Type:      {output_details[0]['dtype']}\")\n",
    "print(f\"   Expected:  [1, 5, 8400] float32\")\n",
    "print(f\"   Format:    [class, x, y, w, h] Ã— 8400 anchors\")\n",
    "\n",
    "# Validate shapes\n",
    "expected_input = [1, 640, 640, 3]\n",
    "expected_output = [1, 5, 8400]\n",
    "\n",
    "input_shape = list(input_details[0]['shape'])\n",
    "output_shape = list(output_details[0]['shape'])\n",
    "\n",
    "input_ok = input_shape == expected_input\n",
    "output_ok = output_shape == expected_output\n",
    "\n",
    "print(\"\\nğŸ” Validation Results:\")\n",
    "print(f\"   Input shape:  {'âœ… PASS' if input_ok else 'âŒ FAIL'}\")\n",
    "print(f\"   Output shape: {'âœ… PASS' if output_ok else 'âŒ FAIL'}\")\n",
    "\n",
    "if input_ok and output_ok:\n",
    "    print(\"\\nâœ… MODEL FORMAT VERIFIED - Ready for Android deployment!\")\n",
    "    print(\"   â†’ This model is compatible with your Android app\")\n",
    "else:\n",
    "    print(\"\\nâŒ MODEL FORMAT MISMATCH - DO NOT use this model!\")\n",
    "    print(\"   â†’ Re-export with correct settings\")\n",
    "    if not input_ok:\n",
    "        print(f\"   â†’ Input: Got {input_shape}, expected {expected_input}\")\n",
    "    if not output_ok:\n",
    "        print(f\"   â†’ Output: Got {output_shape}, expected {expected_output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 10: Save Model and Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¾ Saving trained model to Google Drive...\\n\")\n",
    "\n",
    "# Save TFLite model\n",
    "drive_model_path = '/content/drive/MyDrive/stair_yolo_best_float32.tflite'\n",
    "shutil.copy(final_model_path, drive_model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to Google Drive!\")\n",
    "print(f\"   Location: MyDrive/stair_yolo_best_float32.tflite\")\n",
    "print(f\"   Size: {os.path.getsize(drive_model_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save training results\n",
    "print(f\"\\nğŸ’¾ Saving training results...\")\n",
    "drive_results_dir = '/content/drive/MyDrive/stair_training_results'\n",
    "os.makedirs(drive_results_dir, exist_ok=True)\n",
    "\n",
    "# Copy result files\n",
    "results_files = [\n",
    "    'results.png',\n",
    "    'confusion_matrix.png',\n",
    "    'val_batch0_pred.jpg',\n",
    "    'F1_curve.png',\n",
    "    'PR_curve.png',\n",
    "    'P_curve.png',\n",
    "    'R_curve.png'\n",
    "]\n",
    "\n",
    "for filename in results_files:\n",
    "    src = f'{results_dir}/{filename}'\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, f'{drive_results_dir}/{filename}')\n",
    "        print(f\"   âœ… {filename}\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: MyDrive/stair_training_results/\")\n",
    "print(f\"\\nğŸ‰ Training pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CELL 11: Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ TRAINING COMPLETE - READY FOR ANDROID DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Model Performance:\")\n",
    "print(f\"   mAP@50:     {metrics.box.map50*100:.2f}%\")\n",
    "print(f\"   mAP@50-95:  {metrics.box.map*100:.2f}%\")\n",
    "print(f\"   Precision:  {metrics.box.p*100:.2f}%\")\n",
    "print(f\"   Recall:     {metrics.box.r*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“ Files Created in Google Drive:\")\n",
    "print(f\"   1. stair_yolo_best_float32.tflite  ({os.path.getsize(drive_model_path)/1024/1024:.1f} MB)\")\n",
    "print(f\"   2. stair_training_results/  (plots and metrics)\")\n",
    "\n",
    "print(f\"\\nğŸ”„ Next Steps for Android Integration:\")\n",
    "print(f\"\\n   1ï¸âƒ£  Download Model from Google Drive:\")\n",
    "print(f\"      â†’ Open: drive.google.com\")\n",
    "print(f\"      â†’ Find: stair_yolo_best_float32.tflite\")\n",
    "print(f\"      â†’ Download to: ~/Downloads/\")\n",
    "\n",
    "print(f\"\\n   2ï¸âƒ£  Copy to Android Project:\")\n",
    "print(f\"      â†’ Terminal: cp ~/Downloads/stair_yolo_best_float32.tflite \\\\\")\n",
    "print(f\"                     ~/AndroidStudioProjects/StairVision/app/src/main/assets/\")\n",
    "\n",
    "print(f\"\\n   3ï¸âƒ£  Disable Demo Mode:\")\n",
    "print(f\"      â†’ Edit: app/src/main/java/com/example/stairvision/DetectionConfig.kt\")\n",
    "print(f\"      â†’ Change: ENABLE_DEMO_MODE = false\")\n",
    "\n",
    "print(f\"\\n   4ï¸âƒ£  Rebuild Android App:\")\n",
    "print(f\"      â†’ ./gradlew clean assembleDebug\")\n",
    "print(f\"      â†’ adb install -r app/build/outputs/apk/debug/app-debug.apk\")\n",
    "\n",
    "print(f\"\\n   5ï¸âƒ£  Test Real Stair Detection:\")\n",
    "print(f\"      â†’ Launch app on Android device\")\n",
    "print(f\"      â†’ Point camera at REAL stairs\")\n",
    "print(f\"      â†’ Verify: \\\"STAIRS DETECTED âœ…\\\" appears\")\n",
    "print(f\"      â†’ Check: Audio navigation works\")\n",
    "print(f\"      â†’ Confirm: Colored bounding boxes show\")\n",
    "\n",
    "print(f\"\\nâœ… Success Criteria:\")\n",
    "print(f\"   â€¢ Detects real stairs (not images)\")\n",
    "print(f\"   â€¢ Confidence: 40-95%\")\n",
    "print(f\"   â€¢ Audio speaks stair type and distance\")\n",
    "print(f\"   â€¢ Bounding boxes colored by urgency\")\n",
    "print(f\"   â€¢ No false positives on non-stairs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ Ready for CS663 Project Demo!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
